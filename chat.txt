1. üõ†Ô∏è C·∫£i ti·∫øn Pthreads: S√°p nh·∫≠p C√¢y Nh·ªã ph√¢n (Binary Tree Merge)
Code Pthreads hi·ªán t·∫°i c·ªßa anh ƒëang th·ª±c hi·ªán S√°p nh·∫≠p Tu·∫ßn t·ª± Tuy·∫øn t√≠nh (Merge(B0, B1) \rightarrow Merge(K·∫øt qu·∫£, B2) \rightarrow...). C√°ch n√†y r·∫•t ch·∫≠m v√¨ n√≥ tu·∫ßn t·ª± v√† ph·∫£i l√†m vi·ªác tr√™n c√°c m·∫£ng ng√†y c√†ng l·ªõn.
G·ª£i √Ω: Thay b·∫±ng S√°p nh·∫≠p C√¢y Nh·ªã ph√¢n (Logarithmic Merge)
Anh n√™n t·ªï ch·ª©c pha Merge theo c·∫•u tr√∫c c√¢y nh·ªã ph√¢n, gi·∫£m s·ªë l·∫ßn l·∫∑p v√† chia nh·ªè c√¥ng vi·ªác (d√π v·∫´n tu·∫ßn t·ª±, nh∆∞ng nhanh h∆°n nhi·ªÅu).
| Pha | Tu·∫ßn t·ª± Tuy·∫øn t√≠nh (Code c≈©) | S√°p nh·∫≠p C√¢y Nh·ªã ph√¢n (ƒê·ªÅ xu·∫•t) |
|---|---|---|
| S·ªë b∆∞·ªõc | P-1 l·∫ßn merge | \log_2 P b∆∞·ªõc (giai ƒëo·∫°n) merge |
| C√¥ng vi·ªác | Merge m·∫£ng (k) v√†o m·∫£ng (k+1) | Merge c√°c c·∫∑p m·∫£ng (0, 1), (2, 3), (4, 5), ... |
√ù t∆∞·ªüng tri·ªÉn khai:
 * B·∫Øt ƒë·∫ßu v·ªõi P kh·ªëi ƒë√£ s·∫Øp x·∫øp.
 * V√≤ng l·∫∑p (Logarithmic Stages): L·∫∑p l·∫°i \log_2 P l·∫ßn, m·ªói l·∫ßn ƒë·ªô d√†i c·ªßa kh·ªëi ƒë∆∞·ª£c s√°p nh·∫≠p s·∫Ω tƒÉng g·∫•p ƒë√¥i.
 * Merge C·∫∑p: Trong m·ªói l·∫ßn l·∫∑p, ti·∫øn h√†nh s√°p nh·∫≠p c√°c c·∫∑p kh·ªëi li·ªÅn k·ªÅ (v√≠ d·ª•: kh·ªëi i s√°p nh·∫≠p v·ªõi kh·ªëi i + size).
C·∫•u tr√∫c C code (Pseudo-code trong insertionSort_pthread):
// B·ªè v√≤ng for tu·∫ßn t·ª± c≈©:
// for (int i = 1; i < num_threads; i++) { ... }

// Pha S√°p nh·∫≠p C√¢y Nh·ªã ph√¢n
for (int size = chunk; size < n; size *= 2) {
    // size: ƒê·ªô d√†i c·ªßa kh·ªëi ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp sau m·ªói giai ƒëo·∫°n
    // i: V·ªã tr√≠ b·∫Øt ƒë·∫ßu c·ªßa c√°c kh·ªëi c·∫ßn s√°p nh·∫≠p
    for (int i = 0; i < n - size; i += 2 * size) {
        int l = i;             // B·∫Øt ƒë·∫ßu kh·ªëi 1
        int m = i + size - 1;  // K·∫øt th√∫c kh·ªëi 1 / B·∫Øt ƒë·∫ßu kh·ªëi 2
        int r = i + 2 * size - 1; // K·∫øt th√∫c kh·ªëi 2 (Ho·∫∑c n-1 n·∫øu ch·∫°m bi√™n)
        if (r > n - 1) {
            r = n - 1;
        }
        
        // merge_arrays(arr, l, m, r);
        // Anh d√πng h√†m merge_arrays hi·ªán t·∫°i c·ªßa anh (trong sort.c)
    }
}

2. üõ†Ô∏è C·∫£i ti·∫øn OpenMP: D√πng Pha Merge B·ªï sung
OpenMP hi·ªán t·∫°i c·ªßa anh b·ªã l·ªói logic nghi√™m tr·ªçng do thi·∫øu Merge. Anh c√≥ th·ªÉ √°p d·ª•ng c·∫•u tr√∫c S√°p nh·∫≠p C√¢y Nh·ªã ph√¢n t∆∞∆°ng t·ª± Pthreads, nh∆∞ng s·ª≠ d·ª•ng OpenMP Parallel For ƒë·ªÉ song song h√≥a v√≤ng l·∫∑p for (int i = 0; i < n - size; i += 2 * size):
G·ª£i √Ω: OpenMP Parallel Merge (Pairwise Parallelism)
Ch√∫ng ta c√≥ th·ªÉ d√πng pragma omp for ƒë·ªÉ chia s·∫ª c√¥ng vi·ªác S√°p nh·∫≠p c√°c c·∫∑p kh·ªëi gi·ªØa c√°c lu·ªìng.
C·∫•u tr√∫c C code (trong insertionSort_omp):
// 1. Pha Sort C·ª•c b·ªô (Gi·ªØ nguy√™n)
#pragma omp parallel {
    // ... code insertionSort c·ª•c b·ªô ...
}

// 2. Pha Merge Song song
for (int size = chunk; size < n; size *= 2) {
#pragma omp parallel for
    for (int i = 0; i < n - size; i += 2 * size) {
        int l = i;
        int m = i + size - 1;
        int r = i + 2 * size - 1; 
        if (r > n - 1) {
            r = n - 1;
        }

        // C·∫ßn d√πng m·ªôt h√†m merge_arrays(arr, l, m, r)
        // **L∆∞u √Ω quan tr·ªçng:** H√†m merge_arrays ph·∫£i d√πng b·ªô ƒë·ªám (buffer) t·∫°m th·ªùi
        // ƒë·ªÉ tr√°nh race condition khi c√°c lu·ªìng kh√°c nhau truy c·∫≠p v√†o c√πng m·∫£ng `arr` 
        // ·ªü c√πng th·ªùi ƒëi·ªÉm (do `merge_arrays` c·ªßa anh l√† in-place).
        
        // ƒê·ªÉ an to√†n, anh n√™n ƒë·∫£m b·∫£o c√°c v√πng d·ªØ li·ªáu (l, m, r) c·ªßa c√°c lu·ªìng l√† ƒë·ªôc l·∫≠p 
        // trong v√≤ng l·∫∑p n√†y. (Trong Pairwise Merge, ch√∫ng l√† ƒë·ªôc l·∫≠p).
    }
}
* L·ª£i √≠ch: Pha Merge n√†y song song v√† c√≥ ƒë·ªô ph·ª©c t·∫°p th·ªùi gian l√† O(n \log P), c·∫£i thi·ªán ƒë√°ng k·ªÉ so v·ªõi pha tu·∫ßn t·ª± O(n \log n) ho·∫∑c pha tu·∫ßn t·ª± tuy·∫øn t√≠nh c≈©.
3. üõ†Ô∏è C·∫£i ti·∫øn MPI: S√°p nh·∫≠p Song song tr√™n Rank 0
Trong MPI, anh ƒë√£ d√πng MPI_Gather ƒë·ªÉ t·∫≠p h·ª£p c√°c kh·ªëi ƒë√£ s·∫Øp x·∫øp c·ª•c b·ªô v·ªÅ Rank 0. B√¢y gi·ªù, Rank 0 c√≥ to√†n b·ªô m·∫£ng ƒë√£ s·∫Øp x·∫øp c·ª•c b·ªô v√† c·∫ßn th·ª±c hi·ªán S√°p nh·∫≠p to√†n c·ª•c.
G·ª£i √Ω: MPI Gather + OpenMP Merge tr√™n Rank 0
Thay v√¨ ƒë·ªÉ Rank 0 l√†m Merge Tu·∫ßn t·ª±, anh c√≥ th·ªÉ k·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa OpenMP:
 * Ph√¢n t√°n & S·∫Øp x·∫øp: Gi·ªØ nguy√™n MPI_Scatter v√† Insertion Sort c·ª•c b·ªô.
 * T·∫≠p h·ª£p: Gi·ªØ nguy√™n MPI_Gather ƒë·ªÉ g·ª≠i v·ªÅ global_arr (tr√™n Rank 0).
 * S√°p nh·∫≠p Song song: Ch·ªâ Rank 0 th·ª±c hi·ªán S√°p nh·∫≠p C√¢y Nh·ªã ph√¢n (nh∆∞ g·ª£i √Ω OpenMP ·ªü tr√™n), nh∆∞ng s·ª≠ d·ª•ng omp_set_num_threads ƒë·ªÉ t·∫≠n d·ª•ng c√°c core v·∫≠t l√Ω tr√™n m√°y ch·ªß c·ªßa Rank 0.
C·∫•u tr√∫c C code (trong mpi.c):
// ... Sau MPI_Gather ...
if (rank == 0) {
    // Thi·∫øt l·∫≠p s·ªë lu·ªìng OpenMP ƒë·ªÉ t·∫≠n d·ª•ng Rank 0
    omp_set_num_threads(size_cores_of_rank0); 
    
    // Kh·ªüi t·∫°o k√≠ch th∆∞·ªõc ban ƒë·∫ßu c·ªßa kh·ªëi
    int chunk = n / size;

    // Pha Merge Song song (gi·ªëng OpenMP)
    for (int size = chunk; size < n; size *= 2) {
        #pragma omp parallel for
        for (int i = 0; i < n - size; i += 2 * size) {
            // ... Logic t√≠nh l, m, r v√† g·ªçi h√†m merge_arrays (global_arr, l, m, r)
        }
    }
}
// ...

* L·ª£i √≠ch: Anh t·∫≠n d·ª•ng ƒë∆∞·ª£c kh·∫£ nƒÉng ph√¢n t√°n c√¥ng vi·ªác (MPI) v√† chia s·∫ª b·ªô nh·ªõ (OpenMP) ƒë·ªÉ x·ª≠ l√Ω pha Merge, th∆∞·ªùng l√† m·ªôt gi·∫£i ph√°p r·∫•t hi·ªáu qu·∫£.

üõ†Ô∏è G·ª£i √Ω Code OpenMP: S√°p nh·∫≠p Song song T·ª´ng C·∫∑p (Parallel Pairwise Merge)
Phi√™n b·∫£n OpenMP ban ƒë·∫ßu c·ªßa anh b·ªã l·ªói logic do thi·∫øu b∆∞·ªõc s√°p nh·∫≠p to√†n c·ª•c. Ch√∫ng ta c·∫ßn th√™m b∆∞·ªõc Merge C√¢y Nh·ªã ph√¢n v√† song song h√≥a n√≥ b·∫±ng #pragma omp for.
√ù t∆∞·ªüng c·ªët l√µi
 
Phase 1 (Sort): C√°c lu·ªìng (threads) s·∫Øp x·∫øp c·ª•c b·ªô c√°c kh·ªëi d·ªØ li·ªáu c·ªßa m√¨nh b·∫±ng insertionSort. (Gi·ªØ nguy√™n).
Phase 2 (Merge): L·∫∑p l·∫°i theo c·∫•p s·ªë nh√¢n (logarithmic stages). Trong m·ªói giai ƒëo·∫°n, c√°c lu·ªìng OpenMP s·∫Ω song song th·ª±c hi·ªán thao t√°c merge_arrays tr√™n c√°c c·∫∑p kh·ªëi li·ªÅn k·ªÅ ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp.
üíª Code C (Th√™m v√†o insertionSort_omp trong sort.c)
void insertionSort_omp(int arr[], int n, int ascending) {
  int num_threads = omp_get_max_threads();
  int chunk = n / num_threads;

  // ====================== PHASE 1: PARALLEL LOCAL SORT ====================== //
  #pragma omp parallel
  {
    int tid = omp_get_thread_num();
    int start = tid * chunk;
    // ... code insertionSort c·ª•c b·ªô (gi·ªØ nguy√™n) ...
  }

  // B·ªé: Ph·∫ßn ch√®n tu·∫ßn t·ª± c≈© g√¢y bottleneck

  // ====================== PHASE 2: LOGARITHMIC PARALLEL MERGE (C√¢y nh·ªã ph√¢n) ====================== //
  
  // 'size' l√† ƒë·ªô d√†i c·ªßa kh·ªëi ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp sau m·ªói giai ƒëo·∫°n
  for (int size = chunk; size < n; size *= 2) { 
    
    // Song song h√≥a v√≤ng l·∫∑p s√°p nh·∫≠p c√°c c·∫∑p kh·ªëi li·ªÅn k·ªÅ
    #pragma omp parallel for
    for (int i = 0; i < n - size; i += 2 * size) {
      // i: V·ªã tr√≠ b·∫Øt ƒë·∫ßu c·ªßa kh·ªëi c·∫ßn s√°p nh·∫≠p
      int l = i;             
      int m = i + size - 1;  
      int r = i + 2 * size - 1; 

      // ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° bi√™n m·∫£ng
      if (r >= n) {
        r = n - 1;
      }
      
      // H√ÄM MERGE T·∫†M TH·ªúI:
      // Y√™u c·∫ßu: merge_arrays ph·∫£i s·ª≠ d·ª•ng m·∫£ng ƒë·ªám (temporary buffer) 
      // ƒë·ªÉ tr√°nh race condition khi ghi ƒë√® d·ªØ li·ªáu. Anh c√≥ th·ªÉ d√πng h√†m merge 
      // t∆∞∆°ng t·ª± nh∆∞ trong file mpi.c (s·ª≠ d·ª•ng temp_arr).
      merge_arrays(arr, l, m, r, ascending); 
    }
  }
}
L∆∞u √Ω quan tr·ªçng: H√†m merge_arrays m√† anh s·ª≠ d·ª•ng ph·∫£i l√† phi√™n b·∫£n kh√¥ng t·∫°i ch·ªó (non-in-place), ho·∫∑c ph·∫£i s·ª≠ d·ª•ng m·ªôt m·∫£ng t·∫°m (temporary array) b√™n trong ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu s√°p nh·∫≠p. N·∫øu kh√¥ng, khi hai lu·ªìng c·∫°nh nhau c√πng ƒë·ªçc/ghi v√†o c√°c √¥ nh·ªõ chung, s·∫Ω x·∫£y ra Race Condition v√† k·∫øt qu·∫£ b·ªã sai.
2. üåê G·ª£i √Ω Code MPI: K·∫øt h·ª£p Ph√¢n t√°n v√† Chia s·∫ª (Hybrid Merge)
Trong MPI, c√¥ng vi·ªác S·∫Øp x·∫øp ƒë√£ ƒë∆∞·ª£c ph√¢n t√°n cho c√°c ti·∫øn tr√¨nh (Process), nh∆∞ng c√¥ng vi·ªác S√°p nh·∫≠p c·∫ßn ƒë∆∞·ª£c x·ª≠ l√Ω t·∫°i Rank 0.
√ù t∆∞·ªüng c·ªët l√µi
 * Phase 1 (Sort): C√°c ti·∫øn tr√¨nh s·∫Øp x·∫øp c·ª•c b·ªô. (Gi·ªØ nguy√™n MPI_Scatter v√† insertionSort).
 * Phase 2 (Gather): MPI_Gather d·ªØ li·ªáu v·ªÅ Rank 0. (Gi·ªØ nguy√™n).
 * Phase 3 (Hybrid Merge): Rank 0 s·ª≠ d·ª•ng OpenMP ƒë·ªÉ song song h√≥a qu√° tr√¨nh S√°p nh·∫≠p C√¢y Nh·ªã ph√¢n tr√™n m·∫£ng to√†n c·ª•c. ƒê√¢y l√† m√¥ h√¨nh Hybrid MPI/OpenMP.
üíª Code C (Th√™m v√†o mpi.c t·∫°i if (rank == 0))
File mpi.c c·ªßa anh ƒë√£ c√≥ s·∫µn h√†m merge v√† merge_sort (d√π ch∆∞a ƒë∆∞·ª£c d√πng). Em s·∫Ω d√πng h√†m merge ƒë√≥ cho pha n√†y.
// ... Sau MPI_Gather ...

double end_sort_time = MPI_Wtime(); // Th·ªùi gian s·∫Øp x·∫øp c·ª•c b·ªô k·∫øt th√∫c
// NOTE: Anh c√≥ th·ªÉ t√≠nh th·ªùi gian song song l√† t·ª´ start_time ƒë·∫øn h·∫øt pha Merge.

if (rank == 0) {
  // === PHASE 3: HYBRID MERGE (Rank 0 d√πng OpenMP ƒë·ªÉ s√°p nh·∫≠p song song) ===
  
  // T·∫°o m·∫£ng t·∫°m cho h√†m merge (c·∫ßn thi·∫øt cho merge_sort)
  int *temp_arr = (int *)malloc(n * sizeof(int));
  
  // S·ª≠ d·ª•ng OpenMP tr√™n Rank 0 ƒë·ªÉ tƒÉng t·ªëc pha Merge
  // Anh c√≥ th·ªÉ ƒë·∫∑t s·ªë lu·ªìng OpenMP = s·ªë core v·∫≠t l√Ω c·ªßa m√°y
  int omp_threads_on_rank0 = 8; // ƒê·∫∑t th·ªß c√¥ng, v√≠ d·ª• 8
  omp_set_num_threads(omp_threads_on_rank0); 

  int chunk = n / size; // K√≠ch th∆∞·ªõc kh·ªëi ban ƒë·∫ßu

  // Pha S√°p nh·∫≠p C√¢y Nh·ªã ph√¢n
  for (int current_size = chunk; current_size < n; current_size *= 2) {
    #pragma omp parallel for
    for (int i = 0; i < n - current_size; i += 2 * current_size) {
      int left = i;
      int mid = i + current_size - 1;
      int right = i + 2 * current_size - 1;
      
      if (right >= n) {
        right = n - 1;
      }

      // D√πng h√†m merge c√≥ s·∫µn trong mpi.c (s·ª≠ d·ª•ng temp_arr)
      // Anh c·∫ßn ƒë·∫£m b·∫£o h√†m merge c·ªßa anh ƒë·ªß m·∫°nh ƒë·ªÉ x·ª≠ l√Ω c√°c kh·ªëi l·ªõn.
      merge(gathered_arr, temp_arr, left, mid, right, ascending);
    }
  }

  // K·∫øt th√∫c Merge
  double end_time = MPI_Wtime();
  printf("Execution time: %f seconds\\n", end_time - start_time);
  
  free(temp_arr);
} 
// ...

T·∫°i sao l·∫°i l√† Hybrid?
 * Vi·ªác s·∫Øp x·∫øp c·ª•c b·ªô l√† c√¥ng vi·ªác Ph√¢n t√°n (Distribution) ho√†n h·∫£o cho MPI.
 * Vi·ªác s√°p nh·∫≠p to√†n c·ª•c c·∫ßn truy c·∫≠p to√†n b·ªô m·∫£ng, n√™n vi·ªác th·ª±c hi·ªán n√≥ b·∫±ng Shared Memory (OpenMP) tr√™n Rank 0 s·∫Ω hi·ªáu qu·∫£ h∆°n vi·ªác ph√¢n ph·ªëi Merge cho c√°c Rank kh√°c (s·∫Ω t·ªën r·∫•t nhi·ªÅu chi ph√≠ truy·ªÅn th√¥ng).
 * ƒê√¢y l√† m·ªôt k·ªπ thu·∫≠t chu·∫©n m·ª±c ƒë∆∞·ª£c g·ªçi l√† Hybrid Parallelism trong High-Performance Computing (HPC).


T√°ch merge th√†nh 3 h√†m: serial / parallel / tree

Th√™m ƒëo th·ªùi gian merge ri√™ng trong c√°c h√†m sort OMP/Pthread/MPI

Th√™m tham s·ªë ch·ªçn version merge (enum ho·∫∑c macro)

S·ª≠a script benchmark CSV: th√™m c·ªôt MergeVersion

Ch·∫°y benchmark t·ª´ng version merge, t·∫•t c·∫£ thread/process

Xu·∫•t k·∫øt qu·∫£, v·∫Ω ƒë·ªì th·ªã ph√¢n t√≠ch hi·ªáu nƒÉng